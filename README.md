# Presto - non-uniform data distribution benchmarking

Project to create a data distribution profile for data sets
with the goal of interpolating post-compression non-uniform
data distributions to model large scale data sets.

## Running the code

Generating a non-uniform data distribution based on a given dataset involves a pipeline of 
three utilies:
* `data_dist.py`: generate a non-uniform data distribtion by compressing a dataset
* `filter_proc_size`: parse json ouput of specific compressor to extract process id and size
* `grid_interp.py`: create an interpolation of the input process list and size scaled in each dimension

The following example use the [hurricane Isabelle cloud dataset](https://www.earthsystemgrid.org/dataset/isabeldata/file.html).  Download a copy of one of the models generated by WRF, in this case hour 48, 
and uncompress the downloaded data:
```
wget https://www.earthsystemgrid.org/api/v1/dataset/isabeldata/file/CLOUDf48.bin.gz
gunzip CLOUDf48.bin.gz
```

The pipeline creates a non-unform distribution of data over eight processes (2x2x2) using sz 
compression, filters the procid and size values, and then doubles each process
dimension (2x2x2) using tri-linear interpolation over the given date. This will generate an 
interpolated data distribution across 64 process (an 4x4x4 process grid).

```
./runcmd ./data_dist.py  CLOUDf48.bin \
                         --shape 100x500x500 \
                         --reshape 50x250x250 \
                         -c sz -j | \
./filter_proc_size | \
./grid_interp.py -d 2x2x2 -s 2x2x2
```


The `data_dist.py` script 
takes the `CLOUDf48.bin` file as input, defines its given shape as 100x500x500 and then splits it into
8 parts of 50x250x250 each.  Each part is compressed using sz and JSON output is produced that includes
many metrics related to the compression. The `data_dist.py` script is run via a wrapper that invokes
the code in the context of the libpressio container.

The `filter_proc_size` script parses the JSON and extracts the process ID and compressed data set size.
It prints the result of each `procid size`, one per line. In this case, that produces 8 lines for a 
2x2x2 process grid.

The `grid_interp.py` takes the non-uniform distribution over 8 processes `-d 2x2x2` and produces a 2x
scaling in each dimension `-s 2x2x2`.  The resulting output generates a data distribution across
64 processes each on a separate line formatted as `procid size`.

Note: the `data_dist.py` non-uniform generator works in the dimensions of the input dataset using 
(slice x row x col). The `grid_interp.py` interpolation utility works in the dimensions of the process
grid using familar X,Y,Z row-major indexing.

## Running the h5bench

In order to test the hdf5 performance we use h5bench with a new data distribution test. This 
test loads the distribution generated above and instantiates memory footprint for individual
rank processes.  It then uses standard h5bench testing configurations.

The new data distribution benchmark is on a feature branch of a fork of h5bench

https://github.com/jprorama/h5bench/tree/feat-data-dist

Cloud the fork and build h5bench following standard steps.

Create a benchmark like the following, not the DATA_DIST_PATH and DATA_DIST_SCALE reference 
the distribution above and any scaling to apply to the memory allocation, to adjust for partical data size of benchmark.
```
{
    "mpi": {
        "command": "mpirun",
        "ranks": "8",
        "configuration": "-n 8 --tag-output"
    },
    "vol": {
    },
    "file-system": {
    },
    "directory": "storage",
    "benchmarks": [
        {
            "benchmark": "write_var_data_dist",
            "file": "test.h5",
            "configuration": {
                "MEM_PATTERN": "CONTIG",
                "FILE_PATTERN": "CONTIG",
                "TIMESTEPS": "5",
                "DELAYED_CLOSE_TIMESTEPS": "2",
                "COLLECTIVE_DATA": "YES",
                "COLLECTIVE_METADATA": "YES",
                "EMULATED_COMPUTE_TIME_PER_TIMESTEP": "1 s",
                "NUM_DIMS": "1",
                "DIM_1": "524288",
                "STDEV_DIM_1": "100000",
                "DIM_2": "1",
                "DIM_3": "1",
                "CSV_FILE": "output.csv",
                "DATA_DIST_PATH": "cloud-8",
                "DATA_DIST_SCALE": "1.0",
                "MODE": "SYNC"
            }
        }
    ]
}
```

## Limitations

* The utilities expect 3D so all inputs should specify values for each dimension.
* The project depends on the containerized [libpressio compresson library](https://github.com/CODARcode/libpressio) which must be installed to run `data_dist.py`.

## Installation

### Installing libpressio

The easiest way to work with libpressio and the large collections of compression utilities it provides
is to install it as a container provded by the [libpressio tutorial](https://github.com/robertu94/libpressio_tutorial/tree/master).  This is easiely done using podman on recent Ubuntu 22.04 releases.

Install podman
```
sudo apt install -y podman 
```

Confirm podman operation with `hello-world`
```
podman run -it hello-world
```

You can run the tutorial container interactively using
```
podman run -it ghcr.io/robertu94/libpressio_tutorial:latest
```

The provided `runcmd` wrapper should take care of all environment setup for any scripts using libpressio
and run from the same directory from which it is invoked.  Data file references are likewise local to 
the current working directory.  All python modules for scripts are provided by and limited to the
libpressio container.

### Installing scipy

The `grid_interp.py` leverages the RegularGridInterpolator from the SciPy package and thus requires
scipy.  This is easily installed via pip:
```
pip install scipy
```
Note: the `grid_interp.py` runs in the systems Python environment, not via the libpressio container.

### Installing hdf5 1.14.x with Subfiling Support

In order to explore different configuration of ranks to file writers in h5bench, we may need to build
a recent release of HDF5 with subfiling support and then build h5bench against that release.

These are the steps to building these tools on Polaris at ALCF.

The first step is to [download the tar.gz of hdf5 1.14.2 from the HDF Group](https://www.hdfgroup.org/downloads/hdf5/source-code/).  The install instructions are provided along with the source in the 
`release_docs/INSTALL` and `INSTALL_CMAKE` for cmake.  The recommended build in approach is to use the 
ctest mechanism. The steps are summarized here for convenience.

Select a base dir for your various projects are located:
```
MYPROJ=$HOME/projects
mkdir -p $MYPROJ
cd $MYPROJ
```

Create a build dir for ctest:
```
HDF5BUILD=$MYPROJ/hdf5-build
mkdir $HDF5BUILD
cd $HDF5BUILD
```

Unpack the downloaded hdf5 source tarball to this directory:
```
tar -xzf ~/Downloads/hdf5-1.14.4-2.tar.gz
```

Copy build support files from the distribution to the top level ctest build dir
```
cp hdf5-1.14.4-2/config/cmake/scripts/CTestScript.cmake .
cp hdf5-1.14.4-2/config/cmake/scripts/HDF5* .
```

Add recommended dependencies in the same directory
```
wget https://github.com/MathisRosenhauer/libaec/releases/download/v1.0.6/libaec-1.0.6.tar.gz
wget https://github.com/madler/zlib/releases/download/v1.3.1/zlib-1.3.1.tar.gz
```

Set up a interactive job on the debug resources for the build.
```
qsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A <project_name>
```

You'll need to cd back into your build dir
```
cd $HDF5BUILD
```

Load the build environment
```
module use /soft/modulefiles
module load spack-pe-base cmake
```

Set the recommended install location from the Install docs
```
export HDF5_HOME=$HDF5BUILD/HDF_Group/HDF5/1.14.4
```

Add these lines to the MPI section of the HDF5options.cmake file
```
  set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_THREADSAFE:BOOL=ON")
  set (ADD_BUILD_OPTIONS "${ADD_BUILD_OPTIONS} -DHDF5_ENABLE_SUBFILING_VFD:BOOL=ON")
```

Note, the h5bench hdf5 build notes and hdf5 subfiling build docs differ in enableing threadsafe. 
We opt to follow the h5bench build notes for hdf5.

Build hdf5 with ctest
```
ctest -S HDF5config.cmake,BUILD_GENERATOR=Unix -D MPI:BOOL=ON -C Release -VV -O hdf5.log
```

Install into the $HDF5_HOME
```
cd build
make install
```

### Building h5bench from repo with data-dist write tests against the hdf5 1.14.x subfiling  build 

Continue from the above build environment and re turn to the project directory

```
cd $MYPROJ/h5bench
```

Note you will need to have cloned h5bench on the login node (in another shell or before the interactive job):
```
git clone https://github.com/jprorama/h5bench
cd h5bench
git checkout feat-data-dist
```

Create the cmake build dir
```
mkdir build && cd build
```

Configure cmake, build, and install
```
cmake -DCMAKE_INSTALL_PREFIX=~/.local ..
make
make install
```

Now you can run h5bench after updating the library path
```
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HDF5_HOME/lib
```
